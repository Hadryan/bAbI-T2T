{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "UVr3zfw-bAJx"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6e6575f1d7a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensor2tensor import problems\n",
    "from tensor2tensor import models\n",
    "from tensor2tensor.bin import t2t_decoder  # To register the hparams set\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor.utils import trainer_lib\n",
    "from tensor2tensor.google.data_generators import babi_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 37
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1520612874605,
     "user": {
      "displayName": "Mostafa Dehghani",
      "photoUrl": "//lh5.googleusercontent.com/-ovSYJK9S1U8/AAAAAAAAAAI/AAAAAAAAALw/GbZ35s0KD_I/s50-c-k-no/photo.jpg",
      "userId": "117544073113991621344"
     },
     "user_tz": 0
    },
    "id": "6EsZtjrObAJ1",
    "outputId": "22ef9414-b1a4-45bb-f99e-d9c893e6f2ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript at 0xae2b050>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "require.config({\n",
    "  paths: {\n",
    "      d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.8/d3.min'\n",
    "  }\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FtqR6jyebAJ6"
   },
   "source": [
    "## HParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Ey-UDpJHbAJ9"
   },
   "outputs": [],
   "source": [
    "# HParams\n",
    "babi_task_id = 'qa3'\n",
    "subset = \"10k\"\n",
    "problem_name = 'babi_qa_sentence_task' + babi_task_id.replace(\"qa\", \"\") + \"_\" + subset\n",
    "model_name = \"babi_r_transformer\"\n",
    "hparams_set = \"r_transformer_act_step_position_timing_tiny\"\n",
    "\n",
    "data_dir = '~/babi/data/' + problem_name \n",
    "\n",
    "# PUT THE MODEL YOU WANT TO LOAD HERE!\n",
    "CHECKPOINT = '~/babi/output/' + problem_name+ '/' + model_name +  '/' + hparams_set + '/'\n",
    "print(CHECKPOINT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5059,
     "status": "ok",
     "timestamp": 1520612879895,
     "user": {
      "displayName": "Mostafa Dehghani",
      "photoUrl": "//lh5.googleusercontent.com/-ovSYJK9S1U8/AAAAAAAAAAI/AAAAAAAAALw/GbZ35s0KD_I/s50-c-k-no/photo.jpg",
      "userId": "117544073113991621344"
     },
     "user_tz": 0
    },
    "id": "7c6JxW_bCu0w",
    "outputId": "07d566f2-e458-4236-aac8-148f04a11b4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "7\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "_TASKS = {\n",
    "      'qa1': 'qa1_single-supporting-fact',\n",
    "      'qa2': 'qa2_two-supporting-facts',\n",
    "      'qa3': 'qa3_three-supporting-facts',\n",
    "      'qa4': 'qa4_two-arg-relations',\n",
    "      'qa5': 'qa5_three-arg-relations',\n",
    "      'qa6': 'qa6_yes-no-questions',\n",
    "      'qa7': 'qa7_counting',\n",
    "      'qa8': 'qa8_lists-sets',\n",
    "      'qa9': 'qa9_simple-negation',\n",
    "      'qa10': 'qa10_indefinite-knowledge',\n",
    "      'qa11': 'qa11_basic-coreference',\n",
    "      'qa12': 'qa12_conjunction',\n",
    "      'qa13': 'qa13_compound-coreference',\n",
    "      'qa14': 'qa14_time-reasoning',\n",
    "      'qa15': 'qa15_basic-deduction',\n",
    "      'qa16': 'qa16_basic-induction',\n",
    "      'qa17': 'qa17_positional-reasoning',\n",
    "      'qa18': 'qa18_size-reasoning',\n",
    "      'qa19': 'qa19_path-finding',\n",
    "      'qa20': 'qa20_agents-motivations'\n",
    "  }\n",
    "\n",
    "meta_data_filename = _TASKS[babi_task_id] + '-meta_data.json'\n",
    "metadata_path = os.path.join(data_dir, meta_data_filename)\n",
    "\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS.data_dir = data_dir\n",
    "\n",
    "truncated_story_length = 130 if babi_task_id == 'qa3' else 70\n",
    "\n",
    "with tf.gfile.GFile(metadata_path, mode='r') as f:\n",
    "  metadata = json.load(f)\n",
    "max_story_length = metadata['max_story_length']\n",
    "max_sentence_length = metadata['max_sentence_length']\n",
    "max_question_length = metadata['max_question_length']\n",
    "\n",
    "print(max_story_length)\n",
    "print(max_sentence_length)\n",
    "print(max_question_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "6ttSu5UhsVP8"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "class bAbiAttentionVisualizer(object):\n",
    "  \"\"\"Helper object for creating Attention visualizations.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self, hparams_set, model_name, data_dir, problem_name, beam_size=1):\n",
    "    story, question, targets, samples, att_mats = build_model(\n",
    "        hparams_set, model_name, data_dir, problem_name, beam_size=beam_size)\n",
    "\n",
    "    # Fetch the problem\n",
    "    babi_problem = problems.problem(problem_name)\n",
    "    encoders = babi_problem.feature_encoders(data_dir)\n",
    "\n",
    "    self.story = story\n",
    "    self.question = question\n",
    "    self.targets = targets\n",
    "    self.att_mats = att_mats\n",
    "    self.samples = samples\n",
    "    self.encoders = encoders\n",
    "\n",
    "  def encode(self, story_str, question_str):\n",
    "    \"\"\"Input str to features dict, ready for inference.\"\"\"\n",
    "    \n",
    "    story_str = babi_qa._normalize_string(story_str)\n",
    "    question_str = babi_qa._normalize_string(question_str)\n",
    "    story = story_str.strip().split('.')\n",
    "    story = [self.encoders[babi_qa.FeatureNames.STORY].encode(sentence) \n",
    "                       for sentence in story[-truncated_story_length:]]\n",
    "    question = self.encoders[babi_qa.FeatureNames.QUESTION].encode(question_str)\n",
    "     \n",
    "    for sentence in story:\n",
    "      for _ in range(max_sentence_length - len(sentence)):\n",
    "        sentence.append(babi_qa.PAD)\n",
    "      assert len(sentence) == max_sentence_length\n",
    "\n",
    "    for _ in range(max_story_length - len(story)):\n",
    "      story.append([babi_qa.PAD for _ in range(max_sentence_length)])\n",
    "\n",
    "    for _ in range(max_question_length - len(question)):\n",
    "      question.append(babi_qa.PAD)\n",
    "\n",
    "    assert len(story) == max_story_length\n",
    "    assert len(question) == max_question_length   \n",
    "\n",
    "    story_flat = [token_id for sentence in story for token_id in sentence]\n",
    "    \n",
    "    batch_story = np.reshape(np.array(story_flat), \n",
    "                             [1, max_story_length, max_sentence_length, 1])\n",
    "    batch_question = np.reshape(np.array(question), \n",
    "                         [1, 1, max_question_length, 1])\n",
    "    return batch_story, batch_question\n",
    "\n",
    "  def decode_story(self, integers):\n",
    "    \"\"\"List of ints to str.\"\"\"\n",
    "    integers = np.squeeze(integers).tolist()\n",
    "    story = []\n",
    "    for sent in integers:\n",
    "      sent_decoded = self.encoders[babi_qa.FeatureNames.STORY].decode_list(sent)\n",
    "      sent_decoded.append('.')\n",
    "      story.append(sent_decoded)\n",
    "    return story\n",
    "  \n",
    "  def decode_question(self, integers):\n",
    "    \"\"\"List of ints to str.\"\"\"\n",
    "    integers = np.squeeze(integers).tolist()\n",
    "    return self.encoders[babi_qa.FeatureNames.QUESTION].decode_list(integers)\n",
    "  \n",
    "  def decode_targets(self, integers):\n",
    "    \"\"\"List of ints to str.\"\"\"\n",
    "    integers = np.squeeze(integers).tolist()\n",
    "    return self.encoders[\"targets\"].decode(integers)\n",
    "\n",
    "  def get_vis_data_from_string(self, sess, story_str, question_str):\n",
    "    \"\"\"Constructs the data needed for visualizing attentions.\n",
    "\n",
    "    Args:\n",
    "      sess: A tf.Session object.\n",
    "      input_string: The input setence to be visulized.\n",
    "\n",
    "    Returns:\n",
    "      Tuple of (\n",
    "          output_string: The answer\n",
    "          input_list: Tokenized input sentence.\n",
    "          output_list: Tokenized answer.\n",
    "          att_mats: Tuple of attention matrices; (\n",
    "              enc_atts: Encoder self attention weights.\n",
    "                A list of `num_layers` numpy arrays of size\n",
    "                (batch_size, num_heads, inp_len, inp_len)\n",
    "\n",
    "          )\n",
    "    \"\"\"\n",
    "    encoded_story, encoded_question = self.encode(story_str, question_str)\n",
    "\n",
    "    # Run inference graph to get the label.\n",
    "    out = sess.run(self.samples, {\n",
    "        self.story: encoded_story,\n",
    "        self.question: encoded_question,\n",
    "    })\n",
    "\n",
    "    # Run the decoded answer through the training graph to get the\n",
    "    # attention tensors.\n",
    "    att_mats = sess.run(self.att_mats, {\n",
    "        self.story: encoded_story,\n",
    "        self.question: encoded_question,\n",
    "        self.targets: np.reshape(out, [1, -1, 1, 1]),\n",
    "    })\n",
    "    \n",
    "    output = self.decode_targets(out)\n",
    "    story_list = self.decode_story(encoded_story)\n",
    "    question_list = self.decode_question(encoded_question)\n",
    "    \n",
    "    return story_list, question_list, output, att_mats\n",
    "\n",
    "\n",
    "def build_model(hparams_set, model_name, data_dir, problem_name, beam_size=1):\n",
    "  \"\"\"Build the graph required to featch the attention weights.\n",
    "\n",
    "  Args:\n",
    "    hparams_set: HParams set to build the model with.\n",
    "    model_name: Name of model.\n",
    "    data_dir: Path to directory contatining training data.\n",
    "    problem_name: Name of problem.\n",
    "    beam_size: (Optional) Number of beams to use when decoding a traslation.\n",
    "        If set to 1 (default) then greedy decoding is used.\n",
    "\n",
    "  Returns:\n",
    "    Tuple of (\n",
    "        inputs: Input placeholder to feed in ids.\n",
    "        targets: Targets placeholder to feed to th when fetching\n",
    "            attention weights.\n",
    "        samples: Tensor representing the ids of the translation.\n",
    "        att_mats: Tensors representing the attention weights.\n",
    "    )\n",
    "  \"\"\"\n",
    "  hparams = trainer_lib.create_hparams(\n",
    "      hparams_set, data_dir=data_dir, problem_name=problem_name)\n",
    "  babi_model = registry.model(model_name)(\n",
    "      hparams, tf.estimator.ModeKeys.EVAL)\n",
    "          \n",
    "  story = tf.placeholder(tf.int32, shape=(\n",
    "      1, max_story_length, max_sentence_length, 1), \n",
    "                            name=babi_qa.FeatureNames.STORY)\n",
    "  question = tf.placeholder(tf.int32, shape=(\n",
    "      1, 1, max_question_length, 1), \n",
    "                            name=babi_qa.FeatureNames.QUESTION)\n",
    "  targets = tf.placeholder(tf.int32, shape=(1, 1, 1, 1), name='targets')\n",
    "  \n",
    "  babi_model({\n",
    "      babi_qa.FeatureNames.STORY: story,\n",
    "      babi_qa.FeatureNames.QUESTION: question,\n",
    "      'targets': targets,\n",
    "  })\n",
    "\n",
    "  # Must be called after building the training graph, so that the dict will\n",
    "  # have been filled with the attention tensors. BUT before creating the\n",
    "  # interence graph otherwise the dict will be filled with tensors from\n",
    "  # inside a tf.while_loop from decoding and are marked unfetchable.\n",
    "  att_mats = get_att_mats(babi_model)\n",
    "\n",
    "  with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
    "    samples = babi_model.infer({\n",
    "       babi_qa.FeatureNames.STORY: story,\n",
    "       babi_qa.FeatureNames.QUESTION: question,\n",
    "    }, beam_size=beam_size)['outputs']\n",
    "\n",
    "  return story, question, targets, samples, att_mats\n",
    "\n",
    "\n",
    "def get_att_mats(babi_model):\n",
    "  \"\"\"Get's the tensors representing the attentions from a build model.\n",
    "\n",
    "  The attentions are stored in a dict on the Transformer object while building\n",
    "  the graph.\n",
    "\n",
    "  Args:\n",
    "    babi_model: Transformer object to fetch the attention weights from.\n",
    "\n",
    "  Returns:\n",
    "  Tuple of attention matrices; (\n",
    "      enc_atts: Encoder self attention weights.\n",
    "        A list of `num_layers` numpy arrays of size\n",
    "        (batch_size, num_heads, inp_len, inp_len)\n",
    "  )\n",
    "  \"\"\"\n",
    "  enc_atts = []\n",
    "  \n",
    "  \n",
    "  prefix = model_name + '/parallel_0_6/'+ model_name + '/body'\n",
    "  postfix = 'multihead_attention/dot_product_attention/attention_weights:0'\n",
    " \n",
    "  for i in range(babi_model.hparams.num_hidden_layers):\n",
    "#     print(babi_model.attention_weights)\n",
    "    layer = 'layer' if i==0 else 'layer_{}'.format(i)\n",
    "  babi_model.\n",
    "    enc_att = tf.get_default_graph().get_tensor_by_name(\n",
    "        '%s/encoder/%s/self_attention/%s' % (prefix,layer, postfix))\n",
    "    print(enc_att)\n",
    "    enc_atts.append(enc_att)\n",
    "    \n",
    "    \n",
    "#   prefix = model_name + '/body'\n",
    "#   postfix = 'multihead_attention/dot_product_attention'\n",
    "#   for i in range(babi_model.hparams.num_hidden_layers):\n",
    "#     print(babi_model.attention_weights)\n",
    "#     layer = 'layer' # if i==0 else 'layer_{}'.format(i)\n",
    "#     enc_att = babi_model.attention_weights[\n",
    "#         '%s/encoder/%s/self_attention/%s' % (prefix,layer, postfix)]\n",
    "#     enc_atts.append(enc_att)\n",
    "\n",
    "  return enc_atts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8UqZTxhpbAJ_"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 921
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8853,
     "status": "ok",
     "timestamp": 1520612889223,
     "user": {
      "displayName": "Mostafa Dehghani",
      "photoUrl": "//lh5.googleusercontent.com/-ovSYJK9S1U8/AAAAAAAAAAI/AAAAAAAAALw/GbZ35s0KD_I/s50-c-k-no/photo.jpg",
      "userId": "117544073113991621344"
     },
     "user_tz": 0
    },
    "id": "trJe-wmz_EFw",
    "outputId": "e3a26a5f-8c61-4878-ed48-af0075d1dff4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-09 16:28:04,108] Setting T2TModel mode to 'eval'\n",
      "[2018-03-09 16:28:04,109] Setting hparams.dropout to 0.0\n",
      "[2018-03-09 16:28:04,110] Setting hparams.layer_prepostprocess_dropout to 0.0\n",
      "[2018-03-09 16:28:04,111] Setting hparams.attention_dropout to 0.0\n",
      "[2018-03-09 16:28:04,111] Setting hparams.symbol_dropout to 0.0\n",
      "[2018-03-09 16:28:04,112] Setting hparams.relu_dropout to 0.0\n",
      "[2018-03-09 16:28:04,116] Using variable initializer: uniform_unit_scaling\n",
      "[2018-03-09 16:28:04,149] Transforming feature 'question' with symbol_modality_40_128.bottom\n",
      "[2018-03-09 16:28:04,249] Transforming feature 'story' with symbol_modality_40_128.bottom\n",
      "[2018-03-09 16:28:04,257] Transforming 'targets' with class_label_modality_40_128.targets_bottom\n",
      "[2018-03-09 16:28:04,260] Building model body\n",
      "[2018-03-09 16:28:06,477] From /google/src/cloud/dehghani/t2t_babi/google3/blaze-bin/third_party/py/tensor2tensor/google/colab.runfiles/google3/third_party/py/tensor2tensor/layers/common_layers.py:498: calling reduce_mean (from google3.third_party.tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "[2018-03-09 16:28:06,522] weightsweightsweightsweightsweightsweights\n",
      "[2018-03-09 16:28:06,522] Tensor(\"babi2l_transformer_encoder_sentence_share_layer_params/parallel_0_6/babi2l_transformer_encoder_sentence_share_layer_params/body/encoder/layer/self_attention/multihead_attention/dot_product_attention/attention_weights:0\", shape=(1, 4, 71, 71), dtype=float32)\n",
      "[2018-03-09 16:28:06,788] weightsweightsweightsweightsweightsweights\n",
      "[2018-03-09 16:28:06,788] Tensor(\"babi2l_transformer_encoder_sentence_share_layer_params/parallel_0_6/babi2l_transformer_encoder_sentence_share_layer_params/body/encoder/layer_1/self_attention/multihead_attention/dot_product_attention/attention_weights:0\", shape=(1, 4, 71, 71), dtype=float32)\n",
      "[2018-03-09 16:28:06,876] weightsweightsweightsweightsweightsweights\n",
      "[2018-03-09 16:28:06,877] Tensor(\"babi2l_transformer_encoder_sentence_share_layer_params/parallel_0_6/babi2l_transformer_encoder_sentence_share_layer_params/body/encoder/layer_2/self_attention/multihead_attention/dot_product_attention/attention_weights:0\", shape=(1, 4, 71, 71), dtype=float32)\n",
      "[2018-03-09 16:28:06,964] weightsweightsweightsweightsweightsweights\n",
      "[2018-03-09 16:28:06,964] Tensor(\"babi2l_transformer_encoder_sentence_share_layer_params/parallel_0_6/babi2l_transformer_encoder_sentence_share_layer_params/body/encoder/layer_3/self_attention/multihead_attention/dot_product_attention/attention_weights:0\", shape=(1, 4, 71, 71), dtype=float32)\n",
      "[2018-03-09 16:28:07,029] Number of all trainable parameters: 208768\n",
      "[2018-03-09 16:28:07,029] Transforming body output with class_label_modality_40_128.top\n",
      "[2018-03-09 16:28:07,101] From /google/src/cloud/dehghani/t2t_babi/google3/blaze-bin/third_party/py/tensor2tensor/google/colab.runfiles/google3/third_party/py/tensor2tensor/layers/common_layers.py:1693: softmax_cross_entropy_with_logits (from google3.third_party.tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "[2018-03-09 16:28:07,136] Greedy Decoding\n",
      "[2018-03-09 16:28:07,177] Using variable initializer: uniform_unit_scaling\n",
      "[2018-03-09 16:28:07,206] Transforming feature 'question' with symbol_modality_40_128.bottom\n",
      "[2018-03-09 16:28:07,224] Transforming feature 'story' with symbol_modality_40_128.bottom\n",
      "[2018-03-09 16:28:07,233] Transforming 'targets' with class_label_modality_40_128.targets_bottom\n",
      "[2018-03-09 16:28:07,243] Building model body\n",
      "[2018-03-09 16:28:07,326] weightsweightsweightsweightsweightsweights\n",
      "[2018-03-09 16:28:07,327] Tensor(\"babi2l_transformer_encoder_sentence_share_layer_params/parallel_0_13/babi2l_transformer_encoder_sentence_share_layer_params/body/encoder/layer/self_attention/multihead_attention/dot_product_attention/attention_weights:0\", shape=(1, 4, 71, 71), dtype=float32)\n",
      "[2018-03-09 16:28:07,604] weightsweightsweightsweightsweightsweights\n",
      "[2018-03-09 16:28:07,605] Tensor(\"babi2l_transformer_encoder_sentence_share_layer_params/parallel_0_13/babi2l_transformer_encoder_sentence_share_layer_params/body/encoder/layer_1/self_attention/multihead_attention/dot_product_attention/attention_weights:0\", shape=(1, 4, 71, 71), dtype=float32)\n",
      "[2018-03-09 16:28:07,710] weightsweightsweightsweightsweightsweights\n",
      "[2018-03-09 16:28:07,710] Tensor(\"babi2l_transformer_encoder_sentence_share_layer_params/parallel_0_13/babi2l_transformer_encoder_sentence_share_layer_params/body/encoder/layer_2/self_attention/multihead_attention/dot_product_attention/attention_weights:0\", shape=(1, 4, 71, 71), dtype=float32)\n",
      "[2018-03-09 16:28:07,815] weightsweightsweightsweightsweightsweights\n",
      "[2018-03-09 16:28:07,815] Tensor(\"babi2l_transformer_encoder_sentence_share_layer_params/parallel_0_13/babi2l_transformer_encoder_sentence_share_layer_params/body/encoder/layer_3/self_attention/multihead_attention/dot_product_attention/attention_weights:0\", shape=(1, 4, 71, 71), dtype=float32)\n",
      "[2018-03-09 16:28:07,889] Number of all trainable parameters: 213928\n",
      "[2018-03-09 16:28:07,890] Transforming body output with class_label_modality_40_128.top\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"babi2l_transformer_encoder_sentence_share_layer_params/parallel_0_6/babi2l_transformer_encoder_sentence_share_layer_params/body/encoder/layer/self_attention/multihead_attention/dot_product_attention/attention_weights:0\", shape=(1, 4, 71, 71), dtype=float32)\n",
      "Tensor(\"babi2l_transformer_encoder_sentence_share_layer_params/parallel_0_6/babi2l_transformer_encoder_sentence_share_layer_params/body/encoder/layer_1/self_attention/multihead_attention/dot_product_attention/attention_weights:0\", shape=(1, 4, 71, 71), dtype=float32)\n",
      "Tensor(\"babi2l_transformer_encoder_sentence_share_layer_params/parallel_0_6/babi2l_transformer_encoder_sentence_share_layer_params/body/encoder/layer_2/self_attention/multihead_attention/dot_product_attention/attention_weights:0\", shape=(1, 4, 71, 71), dtype=float32)\n",
      "Tensor(\"babi2l_transformer_encoder_sentence_share_layer_params/parallel_0_6/babi2l_transformer_encoder_sentence_share_layer_params/body/encoder/layer_3/self_attention/multihead_attention/dot_product_attention/attention_weights:0\", shape=(1, 4, 71, 71), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "visualizer = bAbiAttentionVisualizer(hparams_set, model_name, data_dir, problem_name, beam_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18291,
     "status": "ok",
     "timestamp": 1520612907551,
     "user": {
      "displayName": "Mostafa Dehghani",
      "photoUrl": "//lh5.googleusercontent.com/-ovSYJK9S1U8/AAAAAAAAAAI/AAAAAAAAALw/GbZ35s0KD_I/s50-c-k-no/photo.jpg",
      "userId": "117544073113991621344"
     },
     "user_tz": 0
    },
    "id": "Zv8YmyJubAKD",
    "outputId": "6416770c-7f4d-438d-d49b-825f611b2954"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-09 16:28:09,307] Create CheckpointSaverHook.\n",
      "[2018-03-09 16:28:14,486] Graph was finalized.\n",
      "[2018-03-09 16:28:17,947] Restoring parameters from /cns/vz-d/home/dehghani/babi/output/en-10k_qa2/token/babi_qa_as_storysentquestion2label/babi2l_transformer_encoder_sentence_share_layer_params_transformer_tiny_bs_1024_4hl/model.ckpt-248038\n",
      "[2018-03-09 16:28:27,493] Running local_init_op.\n",
      "[2018-03-09 16:28:27,508] Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "tf.Variable(0, dtype=tf.int64, trainable=False, name='global_step')\n",
    "\n",
    "sess = tf.train.MonitoredTrainingSession(\n",
    "    checkpoint_dir=CHECKPOINT,\n",
    "    save_summaries_secs=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-z70lbPQbAKM"
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Module for postprocessing and displaying tranformer attentions.\n",
    "\n",
    "This module is designed to be called from an ipython notebook.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "import IPython.display as display\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "vis_html = \"\"\"\n",
    "  <span style=\"user-select:none\">\n",
    "    Layer: <select id=\"layer\"></select>\n",
    "    Attention: <select id=\"att_type\">\n",
    "      <option value=\"inp_inp\">Input - Input</option>\n",
    "    </select>\n",
    "  </span>\n",
    "  <div id='vis'></div>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# __location__ = os.path.realpath(\n",
    "#     os.path.join(os.getcwd(), os.path.dirname(__file__)))\n",
    "# vis_js = open(os.path.join(__location__, 'attention.js')).read()\n",
    "\n",
    "vis_js = tf.gfile.Open('attention.js').read()\n",
    "\n",
    "\n",
    "def pad_remover(attention):\n",
    "  inp_inp_atts = attention['inp_inp']\n",
    "  att_array = np.array(inp_inp_atts['att'])\n",
    "  top = inp_inp_atts['top_text']\n",
    "  bot = inp_inp_atts['bot_text']\n",
    "  pad_index = [ i for i, sent in enumerate(top) if sent.startswith('<pad>')]\n",
    "  start = min(pad_index)\n",
    "  end = max(pad_index)\n",
    "  filtered_att_d2 = np.concatenate((att_array[:,:,:start, :],att_array[:,:,end+1:, :]), axis=2)\n",
    "  filtered_att = np.concatenate((filtered_att_d2[:,:,:, :start],filtered_att_d2[:,:,:, end+1:]), axis=3)\n",
    "  filteredtop = top[:start] + top[end+1:]\n",
    "  filteredbot = bot[:start] + bot[end+1:]\n",
    "  filteredtop = [sent.replace('<pad> ', '') for sent in filteredtop]\n",
    "  filteredbot = [sent.replace('<pad> ', '') for sent in filteredbot]\n",
    "  inp_inp_atts['att'] = filtered_att.tolist()\n",
    "  inp_inp_atts['top_text'] = filteredtop\n",
    "  inp_inp_atts['bot_text'] = filteredbot\n",
    "  attention['inp_inp'] = inp_inp_atts\n",
    "  return attention\n",
    "  \n",
    "def show(inp_text, out_text, enc_atts):\n",
    "  enc_att = resize(enc_atts)\n",
    "  attention = _get_attention(\n",
    "      inp_text, out_text, enc_att)\n",
    "  attention = pad_remover(attention)\n",
    "  att_json = json.dumps(attention)\n",
    "  _show_attention(att_json)\n",
    "  return attention\n",
    "\n",
    "\n",
    "def _show_attention(att_json):\n",
    "  display.display(display.HTML(vis_html))\n",
    "  display.display(display.Javascript('window.attention = %s' % att_json))\n",
    "  display.display(display.Javascript(vis_js))\n",
    "\n",
    "\n",
    "def resize(att_mat, max_length=None):\n",
    "  \"\"\"Normalize attention matrices and reshape as necessary.\"\"\"\n",
    "  for i, att in enumerate(att_mat):\n",
    "    # Add extra batch dim for viz code to work.\n",
    "    if att.ndim == 3:\n",
    "      att = np.expand_dims(att, axis=0)\n",
    "    if max_length is not None:\n",
    "      # Sum across different attention values for each token.\n",
    "      att = att[:, :, :max_length, :max_length]\n",
    "      row_sums = np.sum(att, axis=2)\n",
    "      # Normalize\n",
    "      att /= row_sums[:, :, np.newaxis]\n",
    "    att_mat[i] = att\n",
    "  return att_mat\n",
    "\n",
    "\n",
    "def _get_attention(inp_text, out_text, enc_atts):\n",
    "  \"\"\"Compute representation of the attention ready for the d3 visualization.\n",
    "\n",
    "  Args:\n",
    "    inp_text: list of strings, words to be displayed on the left of the vis\n",
    "    out_text: list of strings, words to be displayed on the right of the vis\n",
    "    enc_atts: numpy array, encoder self-attentions\n",
    "        [num_layers, batch_size, num_heads, enc_length, enc_length]\n",
    "\n",
    "  Returns:\n",
    "    Dictionary of attention representations with the structure:\n",
    "    {\n",
    "      'inp_inp': Representations for showing encoder self-attentions\n",
    "    }\n",
    "    and each sub-dictionary has structure:\n",
    "    {\n",
    "      'att': list of inter attentions matrices, one for each attention head\n",
    "      'top_text': list of strings, words to be displayed on the left of the vis\n",
    "      'bot_text': list of strings, words to be displayed on the right of the vis\n",
    "    }\n",
    "  \"\"\"\n",
    "\n",
    "  def get_inp_inp_attention(layer):\n",
    "    att = np.transpose(enc_atts[layer][0], (0, 2, 1))\n",
    "    return [ha.T.tolist() for ha in att]\n",
    "\n",
    "  def get_attentions(get_attention_fn):\n",
    "    num_layers = len(enc_atts)\n",
    "    attentions = []\n",
    "    for i in range(num_layers):\n",
    "      attentions.append(get_attention_fn(i))\n",
    "\n",
    "    return attentions\n",
    "\n",
    "  attentions = {\n",
    "      'inp_inp': {\n",
    "          'att': get_attentions(get_inp_inp_attention),\n",
    "          'top_text': inp_text,\n",
    "          'bot_text': inp_text,\n",
    "      },\n",
    "  }\n",
    "\n",
    "  return attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15960,
     "status": "ok",
     "timestamp": 1520612925770,
     "user": {
      "displayName": "Mostafa Dehghani",
      "photoUrl": "//lh5.googleusercontent.com/-ovSYJK9S1U8/AAAAAAAAAAI/AAAAAAAAALw/GbZ35s0KD_I/s50-c-k-no/photo.jpg",
      "userId": "117544073113991621344"
     },
     "user_tz": 0
    },
    "id": "nT7GG8tmbAKH",
    "outputId": "3b1f3144-69ff-403e-fc1e-8bcb8b033b17"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-09 16:28:34,099] Saving checkpoints for 248038 into /cns/vz-d/home/dehghani/babi/output/en-10k_qa2/token/babi_qa_as_storysentquestion2label/babi2l_transformer_encoder_sentence_share_layer_params_transformer_tiny_bs_1024_4hl/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hallway\n",
      "(4, 1, 4, 71, 71)\n"
     ]
    }
   ],
   "source": [
    "if babi_task_id == 'qa1':\n",
    "#   input_story = \"John travelled to the hallway.Mary journeyed to the bathroom.\"\n",
    "#   input_question = \"Where is John?\" #hallway\n",
    "  \n",
    "  input_story = \"John travelled to the hallway.Mary journeyed to the bathroom.Daniel went back to the bathroom.John moved to the bedroom.\"\n",
    "  input_question = \"Where is Mary?\" #bathroom\n",
    "\n",
    "elif babi_task_id == 'qa2':\n",
    "  input_story = \"Mary got the milk there.John moved to the bedroom.Sandra went back to the kitchen.Mary travelled to the hallway.\"\n",
    "  input_question = \"Where is the milk?\" #hallway\n",
    "  \n",
    "#   input_story = \"Mary got the milk there.John moved to the bedroom.Sandra went back to the kitchen.Mary travelled to the hallway.John got the football there.John went to the hallway.\"\n",
    "#   input_question = \"Where is the football?\" #hallway\n",
    "\n",
    "elif babi_task_id == 'qa3':\n",
    "  input_story = \"Mary got the milk.John moved to the bedroom.Daniel journeyed to the office.John grabbed the apple there.John got the football.John journeyed to the garden.Mary left the milk.John left the football.Daniel moved to the garden.Daniel grabbed the football.Mary moved to the hallway.Mary went to the kitchen.John put down the apple there.John picked up the apple.Sandra moved to the hallway.Daniel left the football there.Daniel took the football.John travelled to the kitchen.Daniel dropped the football.John dropped the apple.John grabbed the apple.John went to the office.Sandra went back to the bedroom.Sandra took the milk.John journeyed to the bathroom.John travelled to the office.Sandra left the milk.Mary went to the bedroom.Mary moved to the office.John travelled to the hallway.Sandra moved to the garden.Mary moved to the kitchen.Daniel took the football.Mary journeyed to the bedroom.Mary grabbed the milk there.Mary discarded the milk.John went to the garden.John discarded the apple there.\"\n",
    "  input_question = \"Where was the apple before the bathroom?\" #office\n",
    "  \n",
    "#   input_story = \"Mary got the milk.John moved to the bedroom.Daniel journeyed to the office.John grabbed the apple there.John got the football.John journeyed to the garden.Mary left the milk.John left the football.Daniel moved to the garden.Daniel grabbed the football.Mary moved to the hallway.Mary went to the kitchen.John put down the apple there.John picked up the apple.Sandra moved to the hallway.Daniel left the football there.Daniel took the football.John travelled to the kitchen.Daniel dropped the football.John dropped the apple.John grabbed the apple.John went to the office.Sandra went back to the bedroom.Sandra took the milk.John journeyed to the bathroom.John travelled to the office.Sandra left the milk.Mary went to the bedroom.Mary moved to the office.John travelled to the hallway.Sandra moved to the garden.Mary moved to the kitchen.Daniel took the football.Mary journeyed to the bedroom.Mary grabbed the milk there.Mary discarded the milk.John went to the garden.John discarded the apple there.Sandra travelled to the bedroom.Daniel moved to the bathroom.\"\n",
    "#   input_question = \"Where was the apple before the hallway?\" #office\n",
    "  \n",
    "  \n",
    "story_text, question_text, output, att_mats = visualizer.get_vis_data_from_string(sess, input_story, input_question)\n",
    "print(output)\n",
    "# print(story_text)\n",
    "# print(question_text)\n",
    "print(np.array(att_mats).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 384
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 524,
     "status": "ok",
     "timestamp": 1520612926326,
     "user": {
      "displayName": "Mostafa Dehghani",
      "photoUrl": "//lh5.googleusercontent.com/-ovSYJK9S1U8/AAAAAAAAAAI/AAAAAAAAALw/GbZ35s0KD_I/s50-c-k-no/photo.jpg",
      "userId": "117544073113991621344"
     },
     "user_tz": 0
    },
    "id": "Ly9n4ffZOqAK",
    "outputId": "a1a9fd60-4d96-4c75-e5b7-f228c44ce417"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript at 0x7f6a16d8fe50>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript at 0x7f6a16d8fe90>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "inp_text = []\n",
    "for sent in story_text:\n",
    "  inp_text.append(' '.join(sent))\n",
    "inp_text.append(' '.join(question_text))\n",
    "out_text = [output]\n",
    "\n",
    "from colabtools import publish\n",
    "def import_js_deps():\n",
    "  publish.script_url(\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.5/require.min.js\")\n",
    "\n",
    "  publish.javascript('''\n",
    "  requirejs.config({\n",
    "      \"paths\": {\n",
    "        \"d3\": \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.8/d3.min\",\n",
    "        \"jquery\": \"//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min\",\n",
    "      }\n",
    "  });\n",
    "  ''')\n",
    "\n",
    "import_js_deps()\n",
    "attention = show(inp_text, out_text, att_mats)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "FtqR6jyebAJ6"
   ],
   "default_view": {},
   "last_runtime": {
    "build_target": "",
    "kind": "local"
   },
   "name": "BabiTransformerVisualization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
